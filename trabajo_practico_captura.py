# -*- coding: utf-8 -*-
"""Trabajo_Práctico_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Pituca89/proyecto_eda/blob/main/Trabajo_Pr%C3%A1ctico_EDA.ipynb

**Propuesta de análisis:**

Mejorar la experiencia de los usuarios de plataformas e-commerce analizando la relevancia del indicador “rating”
y su influencia para que las ventas finalicen en reclamos o cancelaciones dependiendo de las categorías seleccionadas.

[texto del enlace](https://developers.mercadolibre.com.ar/es_ar/metricas)

curl -X GET -H 'Authorization: Bearer $ACCESS_TOKEN'https://api.mercadolibre.com/orders/$ORDER_ID/feedback

"""
import warnings
import requests as req
import pandas as pd
import json

warnings.filterwarnings("ignore")

""" En las siguientes variables guardamos la información que utilizaremos para conectarnos a la API y formatear
la muestra de datos a capturar.
Donde indicaremos el código id y la clase secreta que la aplicación creada en DevCenter nos expuso, con esto 
realizaremos la integración."""

URL_AUTH = "https://auth.mercadolibre.com.ar"
URL_API = "https://api.mercadolibre.com"
APP_ID = "5160542056640612"
CLIENT_SECRET = "L2XERJAzNQKZOme3l4mc6jZyIr93eX6G"
YOUR_URL = "https://www.google.com"
ENDPOINT_AUTH = "authorization"
ENDPOINT_AUTH_TOKEN = "oauth/token"
CODE = "TG-64616dd5b4e0df000189eabc-224825607"
REFRESH_TOKEN = "TG-64616dfb6ea77500012b0274-224825607"
LIMIT_RESPONSE = 10
FILE_CONFIG = 'tp-eda_cfg.json'


# Función que lee el archivo de configuración donde indicamos:
# el nombre de las columnas
# rename de las columnas
# Un flag 0 ó 1 de si se necesitan para el analisis o no
# Nombre del archivo de escritura
def read_config(file):
    try:
        with open(file, 'r') as f:
            return json.loads(f.read())
    except IOError:
        exit(1)
#Función que imputa los atributos nulos o vacios.
def reemplaza_vacios(x, leyenda):
    if not x:
        return leyenda
    else:
        return x

# Función que realiza la integración con la API, se encarga de capturar la información devolviendo el listado de items
# junto con que categoria pertenece cada caso
# Para obtener la informacion de los items primero se accedio a  "/sites/MLA/categories"
# y luego con el id de categoria se obtuvieron los items del dataset
def get_items():
    params = {
        "grant_type": "refresh_token",
        "client_id": APP_ID,
        "client_secret": CLIENT_SECRET,
        "refresh_token": REFRESH_TOKEN,
        "redirect_uri": YOUR_URL
    }
    headers = {
        "accept": "application/json",
        "content-type": "application/x-www-form-urlencoded"
    }
    response = req.post(URL_API + "/" + ENDPOINT_AUTH_TOKEN,
                        headers=headers, params=params, verify=False, timeout=5)

    access_token = response.json().get('access_token')
    refresh_token = response.json().get('refresh_token')
    expires_in = response.json().get('expires_in')

    # Obtenemos las categorías
    URL = URL_API + "/sites/MLA/categories"
    headers = {
        "Authorization": f"Bearer {access_token}"
    }
    response = req.get(URL, headers=headers, timeout=5)
    categories = response.json()
    df_category = pd.DataFrame(categories)

    # Obtenemos las items
    URL = URL_API + "/sites/MLA/search"
    items = pd.DataFrame([])
    for category in categories:
        params = {
            "category": category.get('id'),
            "offset": 0
        }
        while params.get('offset') < LIMIT_RESPONSE + 1:
            response = req.get(URL, params=params, headers=headers, timeout=5)
            if response.ok:
                if len(response.json().get('results')) > 0:
                    new_items = pd.json_normalize(response.json().get('results'))
                    new_items['category'] = category.get('name')
                    items = pd.concat([items, new_items], axis=0, ignore_index=True)
            params.update({'offset': params.get('offset') + 1})
    return items


config = read_config(FILE_CONFIG)
columns = config.get('columns')
# Se arma un array con los nombres de las columnas, indicando selected==1 para quedarnos unicamente
# con las que vamos a utilizar
columns_selected = [column.get('name') for column in columns if column.get('selected') == 1]
# Se arma un diccionario con los nombres de las columnas y su correspondientes nuevos nombres
# Tambien se indica que filtre por selected==1 para que únicamente lo cree sobre las columnas a analizar
columns_renamed = {column.get('name'): column.get('rename') for column in columns if column.get('selected') == 1}

df = get_items()
df = df[columns_selected]
df = df.rename(columns=columns_renamed)

df['seller_level_id'] = df['seller_level_id'].map(lambda x: reemplaza_vacios(x, "No identificado"))
df['power_seller_status'] = df['power_seller_status'].map(lambda x: reemplaza_vacios(x, "sin_categoria"))

df = df[df['condition'] == "new"]
df = df[df['buying_mode'] == "buy_it_now"]
df = df[df['seller_trans_total'] != 0]

# Se guarda la muestra obtenida en un CSV, cuyo nombre se encuentra informado en el archivo de configuración.
df.to_csv(config.get('filename'), index=False, sep=',', encoding='utf-8')
